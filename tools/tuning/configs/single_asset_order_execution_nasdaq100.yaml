project: yacht
entity: yacht
name: SingleAssetOrderExectionNasdaq100
program: tools/tuning/run.py
method: bayes
metric:
  goal: maximize
  name: backtest_on_validation/PA
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - "--config_file_name"
  - "single_asset_order_execution_nasdaq100_ppo.config.txt"
  - "--market_storage_dir"
  - "./storage"
early_terminate:
  type: hyperband
  min_iter: 5
  eta: 2
parameters:
  agent.name:
    distribution: constant
    value: PPO
  agent.policy.activationFn:
    distribution: categorical
    values:
    - Tanh
    - ReLU
  agent.policy.featureExtractor.dropOutP:
    distribution: uniform
    max: 0.7
    min: 0.0
  agent.policy.featureExtractor.name:
    distribution: constant
    value: RecurrentFeatureExtractor
  agent.policy.featureExtractor.featuresDim:
    distribution: categorical
    values:
      - [16, 16, 16]
      - [16, 16, 32]
      - [16, 16, 16, 16]
      - [16, 16, 16, 32]
      - [16, 16, 16, 16, 16]
      - [16, 16, 16, 16, 32]
      - [32, 32, 32]
      - [32, 32, 64]
      - [32, 32, 32, 32]
      - [32, 32, 32, 64]
      - [64, 64, 32]
      - [64, 64, 64]
      - [64, 64, 128]
      - [64, 64, 64, 32]
      - [64, 64, 64, 64]
      - [64, 64, 64, 128]
  agent.policy.featureExtractor.rnnLayerType:
    distribution: categorical
    values:
      - GRU
      - LSTM
  agent.policy.name:
    distribution: categorical
    values:
    - MlpPolicy
  agent.policy.netArch.shared:
    distribution: categorical
    values:
      - []
      - [16]
      - [32]
      - [64]
      - [128]
      - [32, 16]
      - [64, 32]
      - [128, 64]
  agent.policy.netArch.vf:
    distribution: categorical
    values:
      - [4]
      - [16]
      - [32]
      - [64]
  agent.policy.netArch.pi:
    distribution: categorical
    values:
      - [4]
      - [16]
      - [32]
      - [64]
  agent.verbose:
    distribution: constant
    value: "true"
  environment.actionSchema:
    distribution: constant
    value: DiscreteActionScheme
  environment.rewardSchemas:
    distribution: categorical
    values:
      - - name: DecisionMakingRewardSchema
        - name: ActionMagnitudeRewardSchema
          reward_scaling: 0.01
      - - name: DecisionMakingRewardSchema
        - name: ActionMagnitudeRewardSchema
          reward_scaling: 0.05
      - - name: DecisionMakingRewardSchema
        - name: ActionMagnitudeRewardSchema
          reward_scaling: 0.1
      - - name: DecisionMakingRewardSchema
        - name: ActionMagnitudeRewardSchema
          reward_scaling: 0.5
      - - name: DecisionMakingRewardSchema
        - name: ActionMagnitudeRewardSchema
          reward_scaling: 1.
  environment.initialCashPosition:
    distribution: constant
    value: 5000
  environment.nEnvs:
    distribution: constant
    value: 6
  environment.name:
    distribution: constant
    value: OrderExecutionEnvironment-v0
  environment.possibilities:
    distribution: constant
    value: [0, 0.25, 0.5, 0.75, 1]
  input.backtest.deterministic:
    distribution: constant
    value: "true"
  input.backtest.tickers:
    distribution: categorical
    values:
      - [AAPL]
  input.dataset:
    distribution: constant
    value: DayFrequencyDataset
  input.decisionPriceFeature:
    distribution: constant
    value: TP
  input.embargoRatio:
    distribution: constant
    value: 0.03
  input.validationSplitRatio:
    distribution: constant
    value: 0.4
  input.start:
    distribution: constant
    value: 1/8/2016
  input.end:
    distribution: constant
    value: 1/8/2021
  input.features:
    distribution: constant
    value: [Close, Open, High, Low, Volume]
  input.technicalIndicators:
    distribution: categorical
    values:
      - [macd, rsi_30, cci_30, dx_30]
      - [macd, rsi_30]
      - []
  input.includeWeekends:
    distribution: constant
    value: "false"
  input.market:
    distribution: constant
    value: Yahoo
  input.numAssetsPerDataset:
    distribution: constant
    value: 1
  input.periodLength:
    distribution: constant
    value: 1M
  input.scaleOnInterval:
    distribution: constant
    value: 1d
  input.scaler:
    distribution: constant
    value: MinMaxScaler
  input.tickers:
    distribution: constant
    value: [NASDAQ100]
  input.fineTuneTickers:
    distribution: constant
    value: [AAPL]
  input.windowSize:
    distribution: int_uniform
    max: 45
    min: 1
  meta.device:
    distribution: constant
    value: gpu
  meta.experimentTracker:
    distribution: constant
    value: wandb
  meta.logFrequencySteps:
    distribution: constant
    value: 10000
  train.batchSize:
    distribution: q_uniform
    max: 640
    min: 128
    q: 64
  train.clipRange:
    distribution: normal
    mu: 0.35
    sigma: 0.15
  train.vfClipRange:
    distribution: normal
    mu: 0.35
    sigma: 0.2
  train.collectingNSteps:
    distribution: constant
    value: 4096
  train.entropyCoefficient:
    distribution: uniform
    max: 1.
    min: 0.25
  train.totalTimesteps:
    distribution: constant
    value: 1000000
  train.fineTuneTotalTimesteps:
    distribution: constant
    value: 200000
  train.gaeLambda:
    distribution: uniform
    max: 1.
    min: 0.
  train.gamma:
    distribution: uniform
    max: 1.0
    min: 0.95
  train.learningRate:
    distribution: uniform
    max: 0.001
    min: 0.00005
  train.maxGradNorm:
    distribution: uniform
    max: 75
    min: 25
  train.nEpochs:
    distribution: int_uniform
    max: 20
    min: 8
  train.useSde:
    distribution: constant
    value: "false"
  train.trainerName:
    distribution: constant
    value: Trainer
  train.vfCoefficient:
    distribution: uniform
    max: 0.85
    min: 0.4
