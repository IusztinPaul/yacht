project: yacht
entity: yacht
name: SingleAssetOrderExectionNasdaq100
program: tools/tuning/run.py
method: bayes
metric:
  goal: maximize
  name: backtest_on_validation/PA
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - "--config_file_name"
  - "single_asset_order_execution_nasdaq100_ppo.config.txt"
parameters:
  agent.name:
    distribution: constant
    value: PPO
  agent.policy.activationFn:
    distribution: categorical
    values:
    - Tanh
    - ReLU
  agent.policy.featureExtractor.dropOutP:
    distribution: uniform
    max: 0.8
    min: 0.0
  agent.policy.featureExtractor.name:
    distribution: constant
    value: RecurrentFeatureExtractor
  agent.policy.featureExtractor.featuresDim:
    distribution: categorical
    values:
      - [16, 16, 128]
      - [16, 16, 256]
      - [16, 16, 16, 128]
      - [16, 16, 16, 256]
      - [16, 16, 16, 16, 128]
      - [16, 16, 16, 16, 256]
      - [32, 32, 256]
      - [32, 32, 512]
      - [32, 32, 32, 256]
      - [32, 32, 32, 512]
      - [64, 64, 256]
      - [64, 64, 512]
      - [64, 64, 1024]
      - [64, 64, 64, 256]
      - [64, 64, 64, 512]
      - [64, 64, 64, 1024]
  agent.policy.featureExtractor.rnnLayerType:
    distribution: categorical
    values:
      - GRU
      - LSTM
  agent.policy.name:
    distribution: categorical
    values:
    - MlpPolicy
  agent.policy.netArch.shared:
    distribution: categorical
    values:
      - [64, 32]
      - [128]
      - [128, 64]
      - [256]
      - [256, 128]
      - [512]
      - [512, 256]
  agent.policy.netArch.vf:
    distribution: categorical
    values:
      - [16]
      - [32]
      - [64]
  agent.policy.netArch.pi:
    distribution: categorical
    values:
      - [16]
      - [32]
      - [64]
  agent.verbose:
    distribution: constant
    value: "true"
  environment.actionSchema:
    distribution: constant
    value: DiscreteActionScheme
  environment.rewardSchemas:
    distribution: categorical
    values:
      - - name: DecisionMakingRewardSchema
        - name: ActionMagnitudeRewardSchema
          reward_scaling: 0.01
      - - name: DecisionMakingRewardSchema
        - name: ActionMagnitudeRewardSchema
          reward_scaling: 0.05
      - - name: DecisionMakingRewardSchema
        - name: ActionMagnitudeRewardSchema
          reward_scaling: 0.1
      - - name: DecisionMakingRewardSchema
        - name: ActionMagnitudeRewardSchema
          reward_scaling: 0.5
      - - name: DecisionMakingRewardSchema
        - name: ActionMagnitudeRewardSchema
          reward_scaling: 1.
  environment.initialCashPosition:
    distribution: constant
    value: 5000
  environment.nEnvs:
    distribution: constant
    value: 6
  environment.name:
    distribution: constant
    value: OrderExecutionEnvironment-v0
  environment.possibilities:
    distribution: categorical
    values:
      - [0, 0.25, 0.5, 0.75, 1]
      - [0, 0.1, 0.25, 0.5, 0.75, 1]
  input.backtest.deterministic:
    distribution: constant
    value: "true"
  input.backtest.tickers:
    distribution: categorical
    values:
      - [AAPL]
  input.dataset:
    distribution: constant
    value: DayFrequencyDataset
  input.decisionPriceFeature:
    distribution: constant
    value: TP
  input.embargoRatio:
    distribution: constant
    value: 0.03
  input.validationSplitRatio:
    distribution: constant
    value: 0.4
  input.start:
    distribution: constant
    value: 1/8/2016
  input.end:
    distribution: constant
    value: 1/8/2021
  input.features:
    distribution: categorical
    values:
      - [Close, Open, High, Low, Volume]
      - [Close, Open, High, Low]
      - [Close, Open]
  input.technicalIndicators:
    distribution: categorical
    values:
      - [macd, rsi_30, cci_30, dx_30]
      - [macd, rsi_30]
      - [cci_30, dx_30]
      - [macd]
      - [rsi_30]
      - [cci_30]
      - [dx_30]
      - []
  input.includeWeekends:
    distribution: constant
    value: "false"
  input.market:
    distribution: constant
    value: Yahoo
  input.numAssetsPerDataset:
    distribution: constant
    value: 1
  input.periodLength:
    distribution: constant
    value: 1M
  input.scaleOnInterval:
    distribution: constant
    value: 1d
  input.scaler:
    distribution: constant
    value: MinMaxScaler
  input.tickers:
    distribution: constant
    value: [NASDAQ100]
  input.fineTuneTickers:
    distribution: constant
    value: [AAPL]
  input.windowSize:
    distribution: int_uniform
    max: 90
    min: 1
  meta.device:
    distribution: constant
    value: gpu
  meta.experimentTracker:
    distribution: constant
    value: wandb
  meta.logFrequencySteps:
    distribution: constant
    value: 10000
  train.batchSize:
    distribution: int_uniform
    max: 1024
    min: 64
  train.clipRange:
    distribution: normal
    mu: 0.3
    sigma: 0.1
  train.vfClipRange:
    distribution: normal
    mu: 0.3
    sigma: 0.1
  train.collectingNSteps:
    distribution: constant
    value: 6144
  train.entropyCoefficient:
    distribution: uniform
    max: 1.
    min: 0.0
  train.totalTimesteps:
    distribution: constant
    value: 1300000
  train.fineTuneTotalTimesteps:
    distribution: constant
    value: 200000
  train.gaeLambda:
    distribution: uniform
    max: 1.
    min: 0.9
  train.gamma:
    distribution: uniform
    max: 1.0
    min: 0.9
  train.learningRate:
    distribution: uniform
    max: 0.01
    min: 0.0001
  train.maxGradNorm:
    distribution: uniform
    max: 100
    min: 0.5
  train.nEpochs:
    distribution: int_uniform
    max: 10
    min: 5
  train.useSde:
    distribution: constant
    value: "false"
  train.trainerName:
    distribution: constant
    value: Trainer
  train.vfCoefficient:
    distribution: uniform
    max: 1.
    min: 0.5
